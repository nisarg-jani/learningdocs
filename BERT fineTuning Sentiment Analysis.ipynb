{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dff00d34-5d0b-4f72-93e9-501edad8d62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers datasets torch scikit-learn accelerate>=0.26.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f82e45-bdf8-49ba-b739-23922b417c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janinisa\\AppData\\Local\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from datasets import load_dataset, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53ce44d-15de-4180-b20a-5c0b06090a36",
   "metadata": {},
   "source": [
    "## Load and Prepare Your Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95853bd0-54c2-41c3-869e-7e07f2baa4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"Z:\\BD\\Novumgen\\Data\\Business Analysis\\Backup\\Nisarg\\Learning\\DataBase\\Sentiment Analysis Dataset\\Reviews.csv\"\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c185137b-f832-4020-8735-f4fce9171e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>464698</th>\n",
       "      <td>464699</td>\n",
       "      <td>B000UBD88A</td>\n",
       "      <td>A3RIXEWG42PY46</td>\n",
       "      <td>M. Cirrito</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1241395200</td>\n",
       "      <td>Delicious Coffee</td>\n",
       "      <td>I love Senseo's coffee pods.  They make an exc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321007</th>\n",
       "      <td>321008</td>\n",
       "      <td>B000XB2E1E</td>\n",
       "      <td>A3POMB9ZWDEA9H</td>\n",
       "      <td>N. Thomas</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1336262400</td>\n",
       "      <td>Pamela's just doesn't make a bad product</td>\n",
       "      <td>I am a sworn from-scratch baker. With the poss...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>528325</th>\n",
       "      <td>528326</td>\n",
       "      <td>B001DBJK4C</td>\n",
       "      <td>A3LUXJ38SUF422</td>\n",
       "      <td>Kyle Bowles</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1267660800</td>\n",
       "      <td>Very Impressed (Edited 2011-05-07)</td>\n",
       "      <td>Since I started a gluten free diet a few month...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246912</th>\n",
       "      <td>246913</td>\n",
       "      <td>B0029NIGMA</td>\n",
       "      <td>A1M3QODN76DHIJ</td>\n",
       "      <td>ron</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1349913600</td>\n",
       "      <td>Midnight</td>\n",
       "      <td>Midnight loves gravy and this is about the onl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26305</th>\n",
       "      <td>26306</td>\n",
       "      <td>B001EQ4DAM</td>\n",
       "      <td>A2M80SE2YL2LQ3</td>\n",
       "      <td>Karen E. Schwartz</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1293494400</td>\n",
       "      <td>Good Value</td>\n",
       "      <td>I had difficulty finding hazelnuts before the ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId        ProfileName  \\\n",
       "464698  464699  B000UBD88A  A3RIXEWG42PY46         M. Cirrito   \n",
       "321007  321008  B000XB2E1E  A3POMB9ZWDEA9H          N. Thomas   \n",
       "528325  528326  B001DBJK4C  A3LUXJ38SUF422        Kyle Bowles   \n",
       "246912  246913  B0029NIGMA  A1M3QODN76DHIJ                ron   \n",
       "26305    26306  B001EQ4DAM  A2M80SE2YL2LQ3  Karen E. Schwartz   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "464698                     0                       0      4  1241395200   \n",
       "321007                     0                       0      5  1336262400   \n",
       "528325                     4                       4      3  1267660800   \n",
       "246912                     0                       0      5  1349913600   \n",
       "26305                      1                       1      5  1293494400   \n",
       "\n",
       "                                         Summary  \\\n",
       "464698                          Delicious Coffee   \n",
       "321007  Pamela's just doesn't make a bad product   \n",
       "528325        Very Impressed (Edited 2011-05-07)   \n",
       "246912                                  Midnight   \n",
       "26305                                 Good Value   \n",
       "\n",
       "                                                     Text  \n",
       "464698  I love Senseo's coffee pods.  They make an exc...  \n",
       "321007  I am a sworn from-scratch baker. With the poss...  \n",
       "528325  Since I started a gluten free diet a few month...  \n",
       "246912  Midnight loves gravy and this is about the onl...  \n",
       "26305   I had difficulty finding hazelnuts before the ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d5c685-4207-4201-88d4-91c31d4e8fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Score\n",
       "5    363122\n",
       "4     80655\n",
       "1     52268\n",
       "3     42640\n",
       "2     29769\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Score'].value_counts(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5036f1c3-9323-46cb-be47-9162f2470883",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_sentiment(rating):\n",
    "    if rating <= 2:\n",
    "        return 'Negative'\n",
    "    elif rating >= 4:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cc3f4e9-131c-433e-9329-1636f19ba6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Sentiment'] = df['Score'].apply(label_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0d732f5-1917-4cc6-b851-353b175075d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>ProductId</th>\n",
       "      <th>UserId</th>\n",
       "      <th>ProfileName</th>\n",
       "      <th>HelpfulnessNumerator</th>\n",
       "      <th>HelpfulnessDenominator</th>\n",
       "      <th>Score</th>\n",
       "      <th>Time</th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>325305</th>\n",
       "      <td>325306</td>\n",
       "      <td>B0033GMSTY</td>\n",
       "      <td>A20WUW45LIX63G</td>\n",
       "      <td>Myron J. Rutsky \"Mabsy\"</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1303776000</td>\n",
       "      <td>Great morning cup of coffee</td>\n",
       "      <td>I always liked this Donut House coffee in the ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63673</th>\n",
       "      <td>63674</td>\n",
       "      <td>B002IEZJMA</td>\n",
       "      <td>ABMX8XUNPR3LP</td>\n",
       "      <td>Jennifer Sicurella</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1332806400</td>\n",
       "      <td>Not for casual coffee drinkers</td>\n",
       "      <td>Perhaps it's just something about the area I l...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520197</th>\n",
       "      <td>520198</td>\n",
       "      <td>B000F6SWA4</td>\n",
       "      <td>AFEK35INET1GW</td>\n",
       "      <td>Onmyown</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1165881600</td>\n",
       "      <td>NOT what I expected but VERY GOOD!!</td>\n",
       "      <td>I drink A LOT of tea...99% I drink unsweetened...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390810</th>\n",
       "      <td>390811</td>\n",
       "      <td>B00139ZPKM</td>\n",
       "      <td>A3FJ0YTQ4TR4QL</td>\n",
       "      <td>Rebecca K-B \"RAK-B\"</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1349654400</td>\n",
       "      <td>Quality pet food</td>\n",
       "      <td>We were looking for an organic food after the ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>511606</th>\n",
       "      <td>511607</td>\n",
       "      <td>B003R0LKUE</td>\n",
       "      <td>ANNHLLI73NW4B</td>\n",
       "      <td>Brian_in_Tulsa</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1339891200</td>\n",
       "      <td>All but 2 cans in the case were severely damaged</td>\n",
       "      <td>I purchase most of my dog food from Amazon and...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Id   ProductId          UserId              ProfileName  \\\n",
       "325305  325306  B0033GMSTY  A20WUW45LIX63G  Myron J. Rutsky \"Mabsy\"   \n",
       "63673    63674  B002IEZJMA   ABMX8XUNPR3LP       Jennifer Sicurella   \n",
       "520197  520198  B000F6SWA4   AFEK35INET1GW                  Onmyown   \n",
       "390810  390811  B00139ZPKM  A3FJ0YTQ4TR4QL      Rebecca K-B \"RAK-B\"   \n",
       "511606  511607  B003R0LKUE   ANNHLLI73NW4B           Brian_in_Tulsa   \n",
       "\n",
       "        HelpfulnessNumerator  HelpfulnessDenominator  Score        Time  \\\n",
       "325305                     1                       1      5  1303776000   \n",
       "63673                      0                       1      2  1332806400   \n",
       "520197                     1                       1      4  1165881600   \n",
       "390810                     0                       0      5  1349654400   \n",
       "511606                     1                       1      2  1339891200   \n",
       "\n",
       "                                                 Summary  \\\n",
       "325305                       Great morning cup of coffee   \n",
       "63673                     Not for casual coffee drinkers   \n",
       "520197               NOT what I expected but VERY GOOD!!   \n",
       "390810                                  Quality pet food   \n",
       "511606  All but 2 cans in the case were severely damaged   \n",
       "\n",
       "                                                     Text Sentiment  \n",
       "325305  I always liked this Donut House coffee in the ...  Positive  \n",
       "63673   Perhaps it's just something about the area I l...  Negative  \n",
       "520197  I drink A LOT of tea...99% I drink unsweetened...  Positive  \n",
       "390810  We were looking for an organic food after the ...  Positive  \n",
       "511606  I purchase most of my dog food from Amazon and...  Negative  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4818c7c1-e358-4f83-af4f-d32214d04d87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment\n",
       "Positive    443777\n",
       "Negative     82037\n",
       "Neutral      42640\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84fb3851-51cf-46ef-9d3b-2a001e2ae5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =df[['Text','Sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5cc70c2d-7836-4c43-894b-537065335c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janinisa\\AppData\\Local\\Temp\\ipykernel_9016\\150988234.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.rename(columns={'Text':'review_text'},inplace=True)\n"
     ]
    }
   ],
   "source": [
    "data.rename(columns={'Text':'review_text'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37117b4d-8fa5-44be-8460-af2de848985b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janinisa\\AppData\\Local\\Temp\\ipykernel_9016\\1218865742.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['Sentiment'] = data['Sentiment'].map(sentiment_map).astype('int32')\n"
     ]
    }
   ],
   "source": [
    "sentiment_map = {\n",
    "    'Positive' : 1,\n",
    "    'Negative' : 0,\n",
    "    'Neutral' : 2\n",
    "}\n",
    "\n",
    "data['Sentiment'] = data['Sentiment'].map(sentiment_map).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a788289c-2bca-4efb-b808-00b3ff3ee4da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 568454 entries, 0 to 568453\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   review_text  568454 non-null  object\n",
      " 1   Sentiment    568454 non-null  int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 6.5+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " Sentiment\n",
       " 1    443777\n",
       " 0     82037\n",
       " 2     42640\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.info(), data['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7e1cfe-a7d1-457a-8fe4-07c41809ac41",
   "metadata": {},
   "source": [
    "## Sampling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b36a78ef-4978-4bd3-bdc8-c41cf2fc7721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janinisa\\AppData\\Local\\Temp\\ipykernel_9016\\3373112487.py:5: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  data_sampled = data.groupby('Sentiment', group_keys=False).apply(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import resample\n",
    "\n",
    "# Keep class balance proportional\n",
    "sample_size = 3000  # adjust between 1000–5000 depending on your CPU\n",
    "data_sampled = data.groupby('Sentiment', group_keys=False).apply(\n",
    "    lambda x: x.sample(frac=sample_size / len(data), random_state=42)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a9fc315-9660-4d41-bbda-86f44415e3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3000 entries, 525327 to 183351\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_text  3000 non-null   object\n",
      " 1   Sentiment    3000 non-null   int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 58.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None,\n",
       " Sentiment\n",
       " 1    2342\n",
       " 0     433\n",
       " 2     225\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sampled.info(), data_sampled['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a244c5-8c74-4332-a2d3-2e4d55e93bca",
   "metadata": {},
   "source": [
    "## Spliting Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eecc625e-015b-4b7a-b8ac-6dead53f8c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(data_sampled, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8a72dfa3-e81b-46a0-8f07-956097dc0c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2400 entries, 465607 to 468135\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_text  2400 non-null   object\n",
      " 1   Sentiment    2400 non-null   int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 46.9+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 600 entries, 538116 to 72871\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   review_text  600 non-null    object\n",
      " 1   Sentiment    600 non-null    int32 \n",
      "dtypes: int32(1), object(1)\n",
      "memory usage: 11.7+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.info(), test_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb26f7e3-c25e-496c-8431-d58d27f8e3fd",
   "metadata": {},
   "source": [
    "## Converting into HuggingFace Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091523b4-d998-4912-8281-78dcec96e6f9",
   "metadata": {},
   "source": [
    "### Que: Why we have to convert into HuggingFace Dataset format?\n",
    "#### Ans: Work seamlessly with Hugging Face’s tokenizers and data collators, support tokenization and batched mapping, allow fast loading, shuffling, and filtering — even with large datasets, optimized for PyTorch and TensorFlow training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "49f91cdf-e6ee-46bc-b020-f41c7501aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a28e410-cca4-4008-ad76-5d690faa1fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['review_text', 'Sentiment', '__index_level_0__'],\n",
       "     num_rows: 2400\n",
       " }),\n",
       " Dataset({\n",
       "     features: ['review_text', 'Sentiment', '__index_level_0__'],\n",
       "     num_rows: 600\n",
       " }))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ced982f5-cef1-4e86-96a1-796198a7633e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm a ramyun addict and I have always loved ramyun since I was little.  It is the first thing I crave when I'm hungry and it's the only thing I can keep down when I'm hung-over. Ramyun has been and will always be my comfort food although I consider them very unhealthy. When I first spotted the Black Shin Ramyun on Amazon, I thought it was too expensive and didn't want to buy an entire box especially while I've been trying so hard to stay away from ramyun products to reduce my sodium and other toxic intakes. After many fruitless searches at nearby Asian markets to try a few packets, I ended up getting a case from Amazon after several months of hesitation; what can I say...I have to try all ramyun that I notice. Upon trying, I must say that I'm not disappointed at all and this is my new favorite ramen for the moment. The broth is thicker (from the beef bone extract powder) and has a bit of miso flavor very similar to the restaurant quality Japanese (non-instant) miso ramyun. It is definitely not as spicy as the regular Shin ramyun, but it has much more complex flavor and tastes more home-made with bigger chunks of mushrooms, hot peppers, etc. It is unbelievable that the product is MSG free with its play on all my taste buds, and I was very impressed that the product didn't make me feel as tired as most ramyun usually do after consumption. I'm very happy that I still have many packets left, and I'll savor them for all they're worth, which is actually much cheaper than comparable restaurant ramyun and home-made noodle soups.\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['review_text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb080657-7eeb-4947-8324-c589861a6549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['Sentiment'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "53f01cf4-8fe3-476c-a5da-cf853175c226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "465607"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset['__index_level_0__'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "808ad16a-b44a-4bbc-aab9-fc3c81e4b109",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm a ramyun addict and I have always loved ramyun since I was little.  It is the first thing I crave when I'm hungry and it's the only thing I can keep down when I'm hung-over. Ramyun has been and will always be my comfort food although I consider them very unhealthy. When I first spotted the Black Shin Ramyun on Amazon, I thought it was too expensive and didn't want to buy an entire box especially while I've been trying so hard to stay away from ramyun products to reduce my sodium and other toxic intakes. After many fruitless searches at nearby Asian markets to try a few packets, I ended up getting a case from Amazon after several months of hesitation; what can I say...I have to try all ramyun that I notice. Upon trying, I must say that I'm not disappointed at all and this is my new favorite ramen for the moment. The broth is thicker (from the beef bone extract powder) and has a bit of miso flavor very similar to the restaurant quality Japanese (non-instant) miso ramyun. It is definitely not as spicy as the regular Shin ramyun, but it has much more complex flavor and tastes more home-made with bigger chunks of mushrooms, hot peppers, etc. It is unbelievable that the product is MSG free with its play on all my taste buds, and I was very impressed that the product didn't make me feel as tired as most ramyun usually do after consumption. I'm very happy that I still have many packets left, and I'll savor them for all they're worth, which is actually much cheaper than comparable restaurant ramyun and home-made noodle soups.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['review_text'][465607]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff130c1-a609-4b7b-b408-8cd5677e1d8d",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2bdb2bd3-ef73-49e6-b805-e823805fe618",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def tokenize_function(example):\n",
    "    tokenized = tokenizer(\n",
    "        example['review_text'],\n",
    "        truncation=True,\n",
    "        padding = 'max_length',\n",
    "        max_length=128\n",
    "    )\n",
    "    tokenized['labels'] = example['Sentiment']\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e1b62-a76e-4cb3-8d46-dd3082dfe7a5",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "#### Loads the pre-trained BERT tokenizer for 'bert-base-uncased'. “uncased” = it lowercases all text. Creates a tokenizer object you can use to turn text → token IDs.\n",
    "#### example\\['review_text'] → This pulls the text from your dataset — one batch of examples at a time.\n",
    "#### truncation=True → If the text is longer than 128 tokens, cut it off (so model input size stays fixed).\n",
    "#### padding = 'max_length' → While truncation cuts sequences that are too long, padding adds zeros to sequences that are too short to make them all the same length.\n",
    "#### max_length=128 → Each sequence is exactly 128 tokens long (BERT’s max is 512, but 128 is faster and usually fine for sentiment tasks).\n",
    "#### tokenized\\['labels'] = example\\['Sentiment'] → Adds the sentiment value (0, 1, 2) as a new field named 'labels'. Hugging Face’s Trainer automatically looks for this 'labels' key during training — so this step links your target variable to each example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be63fe02-60a8-4e96-9af9-f21b750ca8dc",
   "metadata": {},
   "source": [
    "### Apply tokenization to the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7a7b63eb-8f7e-4b02-b94b-317597cfca70",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████| 2400/2400 [00:04<00:00, 594.39 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████| 600/600 [00:01<00:00, 597.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194127a5-08b8-4f4c-83ec-6cffc02456ee",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "#### .map() applies your function to every example in the dataset.\n",
    "#### batched=True means it processes multiple rows at once (much faster than looping one by one).\n",
    "#### The resulting datasets now contain numeric tensors instead of raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e4a571b-cee4-477b-b54d-16a49d41bcb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['review_text',\n",
       " 'Sentiment',\n",
       " '__index_level_0__',\n",
       " 'input_ids',\n",
       " 'token_type_ids',\n",
       " 'attention_mask',\n",
       " 'labels']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77584efc-8044-489f-b55c-16849900d80a",
   "metadata": {},
   "source": [
    "### Initially we had only 2 cols: review_text & Sentiment\n",
    "### then __index_level_0__ added as we have converted database into huggingface Dataset objects\n",
    "### As we applied tokenization on this we got 4 extra columns\n",
    "#### 1) input_ids : Token IDs of the review text\n",
    "#### 2) token_type_ids : Tell BERT which token belongs to which sentence\n",
    "#### 3) attention_mask : Tells the model which tokens are real text and which tokens are padding. BERT expects input tensors of the same length (e.g., max_length=128), so shorter sentences are padded with zeros. Without attention_mask, BERT would treat padding tokens as real input, which can hurt performance.\n",
    "#### 4) labels : Target sentiment label\n",
    "### we keep only last 4 columns rest are unnecessary for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "362d639f-c2af-4e83-8e4c-a0f5303e1536",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = ['review_text', 'Sentiment', '__index_level_0__']\n",
    "train_dataset = train_dataset.remove_columns(columns_to_remove)\n",
    "test_dataset = test_dataset.remove_columns(columns_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b142c29c-2ce3-484f-b13e-3b75d7f8cddb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       " ['input_ids', 'token_type_ids', 'attention_mask', 'labels'])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.column_names, test_dataset.column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389cc00-0012-4b9e-8a77-4cc8dbb5e363",
   "metadata": {},
   "source": [
    "## Load Pretrained BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "49784d26-0e59-4d8d-ae4f-7435bb011e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "num_labels = len(data['Sentiment'].unique())\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81262b-899a-4974-a7a4-0df557e6a734",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "#### num_labels → This ensures the output layer of BERT matches your number of classes.\n",
    "#### BertForSequenceClassification is a BERT model with an added classification head: Pretrained BERT encoder → \\[CLS] token embedding → Linear layer → Output logits\n",
    "#### from_pretrained('bert-base-uncased') loads the pretrained weights from BERT’s base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec81400-0a55-4289-a68f-b13945ad445c",
   "metadata": {},
   "source": [
    "## Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25903b7a-a037-4840-9d32-28f60cbadb72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\janinisa\\AppData\\Local\\miniconda3\\Lib\\site-packages\\transformers\\training_args.py:1636: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./result',\n",
    "    eval_strategy='steps',\n",
    "    eval_steps= 100,\n",
    "    save_strategy='no',  # Disable saving\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_strategy='steps',  # log every X steps\n",
    "    logging_steps=50,          # log training loss every 50 steps\n",
    "    save_total_limit=0,  # Don't save any checkpoints\n",
    "    no_cuda=True,  # Force CPU if GPU issues exist\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6bed09-118a-486f-92b4-380fe9958180",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "#### 1) output_dir='./result' : Where the model, tokenizer, and training logs will be saved after training. Even if you disable saving (save_strategy='no'), some temporary outputs or metrics may still be stored here.\n",
    "#### 2) eval_strategy='epoch'Determines how often evaluation runs during training.'epoch' → evaluate once after each full pass through the training dataset. Other options: 'steps' (evaluate every N steps) or 'no' (never evaluate).\n",
    "#### 3) eval_steps= 100 → if eval_strategy='steps' then this can b use. Steps per epoch = 3000/8 =375 As evalution is being done after every 100 step here 3-4 times evaluation will be done.\n",
    "#### 5) learning_rate=2e-5 Too high → may diverge; too low → may converge slowly. 2e-5 is a standard starting point for fine-tuning BERT.\n",
    "#### 6) per_device_train_batch_size=8 → Batch size per device (CPU/GPU) during training.\n",
    "#### 7) per_device_eval_batch_size=8 → Batch size per device during evaluation.\n",
    "#### 8) num_train_epochs=1 → Number of passes over the entire training dataset.\n",
    "#### 9) weight_decay=0.01 → L2 regularization applied to model weights. Helps prevent overfitting, especially for small datasets.\n",
    "#### 10) logging_dir='./logs' → Directory where TensorBoard logs are saved. You can visualize training metrics with TensorBoard.\n",
    "#### 11) save_total_limit=0 → Limits the number of saved checkpoints. 0 → don’t save any, consistent with save_strategy='no'.\n",
    "#### 12) no_cuda=True → Forces CPU training, even if a GPU is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c968cbe8-d99c-4643-8585-ac8f94ab2931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7252158-3fe8-48ae-954e-0c0d00500ac8",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "821779d4-4c83-491b-a1c6-9aa0ca9c1bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model= model,\n",
    "    args= training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c61f495-e99a-440c-88e4-9d4606ae90a7",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "#### data_collator=DataCollatorWithPadding(tokenizer=tokenizer): In tokenize_function we use padding of 128 no matter how small the token sequences are, which lead to unnecessary padding. With Data Collator Dynamic Padding is possible. DataCollatorWithPadding is a built-in collator in Hugging Face that automatically pads sequences in a batch to the length of the longest sequence in that batch.\n",
    "#### tokenizer=tokenizer : This will tell DataCollatorWithPadding which token is used for padding. Also tells Padding side: Knows whether to pad on the left or right."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b78b5c-0f08-4f64-8e94-d0216b86c0f0",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "961aee96-ec0d-4056-9a2c-7a52c64224e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='300' max='300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [300/300 24:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.604200</td>\n",
       "      <td>0.544574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.446900</td>\n",
       "      <td>0.402198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.400600</td>\n",
       "      <td>0.410632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=300, training_loss=0.5201594034830729, metrics={'train_runtime': 1500.856, 'train_samples_per_second': 1.599, 'train_steps_per_second': 0.2, 'total_flos': 157868050636800.0, 'train_loss': 0.5201594034830729, 'epoch': 1.0})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0658fe93-bf7e-4fe5-83d4-05121855abbe",
   "metadata": {},
   "source": [
    "## Predict on New Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dfa13998-77f0-462e-b6a5-013b383479b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = [\n",
    "    \"I didn't absolutely love this!\",\n",
    "    \" Worthy the price.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8a9d3d6-0c28-4efe-ab66-b9e71b7985c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer(test_texts,return_tensors='pt',truncation=True,padding=True,max_length=128)\n",
    "outputs = model(**inputs)\n",
    "predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bb5795-9a5f-478f-9f9c-07e7c66d30e6",
   "metadata": {},
   "source": [
    "#### Pedictions seem to be true, but lets evaluate this model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b96c7d-3aff-4823-8717-e8efde658103",
   "metadata": {},
   "source": [
    "## Evaluation of the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50230f17-a64e-46fc-90af-25ad35d73953",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
